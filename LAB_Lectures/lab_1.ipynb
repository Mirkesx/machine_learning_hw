{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "Y = boston.target\n",
    "print(X.shape) #design matrix\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(123) #in produzione si deve fissare un seed\n",
    "torch.random.manual_seed(123);\n",
    "idx = np.random.permutation(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[idx]\n",
    "Y = Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = torch.Tensor(X[50:])\n",
    "Y_training = torch.Tensor(Y[50:])\n",
    "X_testing = torch.Tensor(X[:50])\n",
    "Y_testing = torch.Tensor(Y[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.Tensor(13) #creiamo un tensore di 13 unità\n",
    "theta_0 = torch.Tensor(1) #tensore di una unità (bias)\n",
    "#impostiamo required_grad\n",
    "theta.requires_grad_(True)\n",
    "theta_0.requires_grad_(True)\n",
    "theta.data.normal_(0,0.1) #inizializziamo il tensore con numeri casuali tratti da una\n",
    "#distribuzione normale di media 0 e varianza 0.1\n",
    "theta_0.data.normal_(0,0.1) \n",
    "#media 0 perchè non vogliamo dare un bias ai parametri\n",
    "#varianza piccola attorno allo 0 riduce il bias che diamo ai parametri\n",
    "#le diverse componenti vivono nello stesso spazio\n",
    "\n",
    "print(theta)\n",
    "print(theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(input, theta, theta_0):\n",
    "    return input.mul(theta).sum(1)+theta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=X_training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.mul(theta).sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_training.mul(theta).sum(1)[0]+theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = linear_regression(X_training,theta,theta_0)\n",
    "print(y[:10]) #stampiamo i primi 10 valori predetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(input, target):\n",
    "    return ((input-target)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss(y,Y_training)) #l'unità di misura è dollari al quadrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X_training.mean(0)\n",
    "stds = X_training.std(0)\n",
    "print(means, stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La normalizzazione la effettuiamo utilizzando la media e la deviazione standard del set di training. Tre motivi:\n",
    "- Se ricalcolo le statistiche dal test set potrebbero trovarsi in un altro spazio rispetto a quelli di training \n",
    "- Ricalcolare le statistiche significa che dobbiamo ritrainare i parametri\n",
    "- I dati di test si presentano ad uno ad uno e quindi le statistiche non possono essere calcolate su di essi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epochs = 10 #iterazioni\n",
    "# Passo 1: normalizzazione dei dati\n",
    "means = X_training.mean(0)\n",
    "stds = X_training.std(0)\n",
    "X_training_norm = (X_training-means)/stds\n",
    "X_testing_norm = (X_testing-means)/stds\n",
    "#Passo 2: inizializziamo i pesi come visto in precedenza\n",
    "theta = torch.Tensor(13);\n",
    "theta_0 = torch.Tensor(1);\n",
    "theta.normal_(0,0.01)\n",
    "theta_0.normal_(0,0.01)\n",
    "theta.requires_grad_(True)\n",
    "theta_0.requires_grad_(True)\n",
    "for e in range(epochs):\n",
    "    #Passo 3: calcoliamo le predizioni\n",
    "    y = linear_regression(X_training_norm,theta,theta_0)\n",
    "\n",
    "    #Passo 4: calcoliamo il valore della loss\n",
    "    l = loss(y, Y_training)\n",
    "    #Passo 5: calcoliamo il gradiente della loss rispetto a tutti i parametri\n",
    "    l.backward()\n",
    "    #stampiamo il valore della loss\n",
    "    print(\"Epoch: {}, loss: {:0.2f}\".format(e,l.item()))\n",
    "\n",
    "    #Passo 6: Aggiorniamo i pesi\n",
    "    theta.data.sub_(lr*theta.grad.data)\n",
    "    theta_0.data.sub_(lr*theta_0.grad.data)\n",
    "\n",
    "    #azzeriamo i gradienti per evitare di accumularli\n",
    "    theta.grad.data.zero_()\n",
    "    theta_0.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epochs = 50\n",
    "# Passo 1: normalizzazione dei dati\n",
    "means = X_training.mean(0)\n",
    "stds = X_training.std(0)\n",
    "X_training_norm = (X_training-means)/stds\n",
    "X_testing_norm = (X_testing-means)/stds\n",
    "#Passo 2: inizializziamo i pesi come visto in precedenza\n",
    "theta = torch.Tensor(13);\n",
    "theta_0 = torch.Tensor(1);\n",
    "theta.normal_(0,0.01)\n",
    "theta_0.normal_(0,0.01)\n",
    "theta.requires_grad_(True)\n",
    "theta_0.requires_grad_(True)\n",
    "losses = []\n",
    "for e in range(epochs):\n",
    "    #Passo 3: calcoliamo le predizioni\n",
    "    y = linear_regression(X_training_norm,theta,theta_0)\n",
    "\n",
    "    #Passo 4: calcoliamo il valore della loss\n",
    "    l = loss(y, Y_training)\n",
    "    #Passo 5: calcoliamo il gradiente della loss rispetto a tutti i parametri\n",
    "    l.backward()\n",
    "    #conserviamo il valore della loss\n",
    "    losses.append(l.item())\n",
    "    #Passo 6: Aggiorniamo i pesi\n",
    "    theta.data.sub_(lr*theta.grad.data)\n",
    "    theta_0.data.sub_(lr*theta_0.grad.data)\n",
    "\n",
    "    #azzeriamo i gradienti per evitare di accumularli\n",
    "    theta.grad.data.zero_()\n",
    "    theta_0.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iniziamo calcolando le predizioni del modello dati i pesi allenati\n",
    "yt = linear_regression(X_testing_norm,theta,theta_0)\n",
    "#calcoliamo il valore della loss\n",
    "print(loss(yt, Y_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epochs = 50\n",
    "# Passo 1: normalizzazione dei dati\n",
    "means = X_training.mean(0)\n",
    "stds = X_training.std(0)\n",
    "X_training_norm = (X_training-means)/stds\n",
    "X_testing_norm = (X_testing-means)/stds\n",
    "#Passo 2: inizializziamo i pesi come visto in precedenza\n",
    "theta = torch.Tensor(13);\n",
    "theta_0 = torch.Tensor(1);\n",
    "theta.normal_(0,0.01)\n",
    "theta_0.normal_(0,0.01)\n",
    "theta.requires_grad_(True)\n",
    "theta_0.requires_grad_(True)\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "for e in range(epochs):\n",
    "    #Passo 3: calcoliamo le predizioni\n",
    "    y = linear_regression(X_training_norm,theta,theta_0)\n",
    "\n",
    "    #Passo 4: calcoliamo il valore della loss\n",
    "    l = loss(y, Y_training)\n",
    "    #conserviamo il valore della loss di training\n",
    "    losses_train.append(l.item())\n",
    "    #Passo 5: calcoliamo il gradiente della loss rispetto a tutti i parametri\n",
    "    l.backward()\n",
    "    #Passo 6: Aggiorniamo i pesi\n",
    "    theta.data.sub_(lr*theta.grad.data)\n",
    "    theta_0.data.sub_(lr*theta_0.grad.data)\n",
    "\n",
    "    #azzeriamo i gradienti per evitare di accumularli\n",
    "    theta.grad.data.zero_()\n",
    "    theta_0.grad.data.zero_()\n",
    "\n",
    "    #calcoliamo la loss sul test set\n",
    "    #dato che non dobbiamo calcolare i gradienti\n",
    "    #li disabilitiamo per risparmiare memoria\n",
    "    #questa notazione è di pytorch per disabilitare i gradienti\n",
    "    with torch.set_grad_enabled(False):\n",
    "        y_test = linear_regression(X_testing_norm, theta, theta_0)\n",
    "        loss_test = loss(y_test, Y_testing)\n",
    "        losses_test.append(loss_test.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(losses_train)\n",
    "plt.plot(losses_test)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.legend(['Training','Testing'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esercizio. y = Ax + b. Dove stanno le feature e dove le variabili?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "linear = nn.Linear(20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = list(linear.parameters())\n",
    "print(par[0].shape) # Matrice A [m x n]\n",
    "print(par[1].shape) # Vettore b [m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#costruiamo una matrice di 150 elementi a 20 dimensioni\n",
    "sample_input = torch.rand((150,20))\n",
    "#otteniamo una matrice di 150 elementi a 10 dimensioni\n",
    "sample_output = linear(sample_input)\n",
    "# abbiamo svolto l'operazione y = Ax + b\n",
    "print(sample_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = nn.Linear(13,1)\n",
    "z = linreg(X_training_norm)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer.step() #ottimizzazione <br>\n",
    "optimizer.zero_grad() #sostituisce il data.zero_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Esercizio 1</h1>\n",
    "<p>Si provi ad allenare il regressore sul dataset Boston senza effettuare la normalizzazione dei dati. Se\n",
    "necessario, si modifichino il learning rate e il numero di epoche per portare il modello a convergenza.\n",
    "Si notano differenze durante il training? Il modello allenato è migliore o peggiore? Si monitorino le\n",
    "curve di training e test mediante tensorboard.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        \"\"\"Costruisce un regressore logistico.\n",
    "        Input:\n",
    "        in_size: numero di feature in input (es. 13)\n",
    "        out_size: numero di elementi in output (es. 1)\"\"\"\n",
    "        super(LinearRegressor, self).__init__() #richiamo il costruttore della superclasse\n",
    "        #questo passo è necessario per abilitare alcuni meccanismi automatici dei moduli di PyTorch\n",
    "        self.linear = nn.Linear(in_size,out_size)\n",
    "    def forward(self,x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        result = self.linear(x)\n",
    "        return result\n",
    "    \n",
    "def reset_elements():\n",
    "    boston = load_boston()\n",
    "    X = boston.data\n",
    "    Y = boston.target\n",
    "    np.random.seed(123)\n",
    "    torch.random.manual_seed(123);\n",
    "    idx = np.random.permutation(len(X))\n",
    "    X = X[idx]\n",
    "    Y = Y[idx]\n",
    "    X_training = torch.Tensor(X[50:])\n",
    "    Y_training = torch.Tensor(Y[50:])\n",
    "    X_testing = torch.Tensor(X[:50])\n",
    "    Y_testing = torch.Tensor(Y[:50])\n",
    "    return X_training, Y_training, X_testing, Y_testing\n",
    "\n",
    "def esercizio1(lr, epochs, type, now):\n",
    "    #facciamo training per 5000 epoche in modo da verificare che\n",
    "    #tensorboard viene aggiornato in tempo reale\n",
    "    writer = SummaryWriter('logs/lab_1/esercizio1/{}/{}'.format(type,now))\n",
    "    X_training, Y_training, X_testing, Y_testing = reset_elements()\n",
    "    if type is \"norm\":\n",
    "        #normalizzazione dei dati\n",
    "        means = X_training.mean(0)\n",
    "        stds = X_training.std(0)\n",
    "    else:\n",
    "        means = 0\n",
    "        stds = 1\n",
    "    X_training_norm = (X_training-means)/stds\n",
    "    X_testing_norm = (X_testing-means)/stds\n",
    "    reg = LinearRegressor(13,1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(reg.parameters(),lr=lr)\n",
    "    for e in range(epochs):\n",
    "        reg.train()\n",
    "        output = reg(X_training_norm)\n",
    "        l = criterion(output.view(-1),Y_training)\n",
    "        writer.add_scalar('loss/train', l.item(), global_step=e)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        reg.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_test = reg(X_testing_norm)\n",
    "            l = criterion(y_test.view(-1),Y_testing)\n",
    "            writer.add_scalar('loss/test', l.item(), global_step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S%z\")\n",
    "epochs = 1000\n",
    "lr = 10**-1\n",
    "esercizio1(lr, epochs, \"norm\", now)\n",
    "epochs = 50000\n",
    "lr = 3*(10**-6)\n",
    "esercizio1(lr, epochs, \"not_norm\", now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Senza normalizzazione dopo 50000 epochs ancora non si è andati in convergenza (che è una quantità almeno 250 volte più grande del numero di epoche richieste se usiamo la normalizzazione)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Esercizio 2</h1>\n",
    "<p>\n",
    "    Abbiamo visto come allenare un modello di regressione lineare in maniera imperativa. Per\n",
    "semplificare l'utilizzo del modello, si cotruisca una classe LinearRegressor con i seguenti\n",
    "metodi:\n",
    "<ul>\n",
    "    <li>Costruttore: prende in input il numero di osservazioni . Il costruttore inizializza i pesi del modello\n",
    "(theta e theta_0);</li>\n",
    "    <li>Metodo fit : prende in input i dati\n",
    "e le etichette\n",
    "per effettuare il training. Il metodo prende in\n",
    "input anche i parametri lr e epochs che indicano il learning rate e il numero di epoche. I valori di default per questi due\n",
    "parametri sono rispettivamente\n",
    "e\n",
    ". Il metodo fit calcola medie e deviazioni standard di\n",
    "e conserva tali valori\n",
    "per usi futuri, normalizza i dati\n",
    "ed effettua l'allenamento del modello. Ad ogni epoca, viene stampato il valore della loss;</li>\n",
    "    <li>Metodo predict : prende in input i dati . Il metodo normalizza i dati\n",
    "utilizzando le medie e le deviazioni standard\n",
    "precedentemente salvate, poi predice e restituisce le etichette predette dal modello sui dati ;</li>\n",
    "    <li>Metodo score : prende in input i dati\n",
    "e le etichette . Il metodo utilizza predict per predire le etichette a partire dai\n",
    "dati\n",
    ", poi calcola e restituisce il valore della loss calcolata utilizzando le etichette predette e le etichette fornite\n",
    ";</li>\n",
    "</ul>\n",
    "E' possibile inserire altri metodi privati (devono iniziare per _ , ad esempio \\_loss ) per rendere la computazione modulare.\n",
    "Utilizzare l'oggetto per:\n",
    "<ul>\n",
    "    <li>\n",
    "Allenare il modello di regressione lineare sui dati di training;</li>\n",
    "    <li>\n",
    "Calcolare la loss di training mediante il metodo score ;</li>\n",
    "    <li>\n",
    "Predire le etichette di test mediante il metodo predict ;</li>\n",
    "    <li>\n",
    "Calcolare la loss di test mediante il metodo score .</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def reset_elements():\n",
    "    boston = load_boston()\n",
    "    X = boston.data\n",
    "    Y = boston.target\n",
    "    np.random.seed(123)\n",
    "    torch.random.manual_seed(123);\n",
    "    idx = np.random.permutation(len(X))\n",
    "    X = X[idx]\n",
    "    Y = Y[idx]\n",
    "    X_training = torch.Tensor(X[50:])\n",
    "    Y_training = torch.Tensor(Y[50:])\n",
    "    X_testing = torch.Tensor(X[:50])\n",
    "    Y_testing = torch.Tensor(Y[:50])\n",
    "    return X_training, Y_training, X_testing, Y_testing\n",
    "\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        \"\"\"Costruisce un regressore lineare.\n",
    "        Input:\n",
    "        in_size: numero di feature in input (es. 13)\n",
    "        out_size: numero di elementi in output (es. 1)\"\"\"\n",
    "        super(LinearRegressor, self).__init__() #richiamo il costruttore della superclasse\n",
    "        #questo passo è necessario per abilitare alcuni meccanismi automatici dei moduli di PyTorch\n",
    "        self.linear = nn.Linear(in_size,out_size)\n",
    "        self.means = 0\n",
    "        self.stds = 1\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.writer = SummaryWriter('logs/esercizio2/')\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        result = self.linear(x)\n",
    "        return result\n",
    "    \n",
    "    def _norm(self, X):\n",
    "        return (X-self.means)/self.stds\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(self._norm(X)).view(-1)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        return self.criterion(self.predict(X),Y).item()\n",
    "        \n",
    "    \n",
    "    def fit(self, X, Y, lr=0.01, epochs=100):\n",
    "        self.means = X.mean(0)\n",
    "        self.stds = X.std(0)\n",
    "        X_norm = self._norm(X)\n",
    "        optimizer = torch.optim.SGD(self.parameters(),lr=lr)\n",
    "        for e in range(epochs):\n",
    "            self.train()\n",
    "            output = self.forward(X_norm)\n",
    "            l = self.criterion(output.view(-1),Y)\n",
    "            print(\"loss/fit: {}\".format(l.item()))\n",
    "            self.writer.add_scalar('loss/fit', l.item(), global_step=e)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegressor(13,1)\n",
    "X_training, Y_training, X_testing, Y_testing = reset_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, Y_training, X_testing, Y_testing = reset_elements()\n",
    "reg.fit(X_training, Y_training, 0.1, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.score(X_training, Y_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = reg.predict(X_testing)\n",
    "print(predictions)\n",
    "print(Y_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.score(X_testing, Y_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Esercizio 3</h1>\n",
    "<p>\n",
    "    Si consideri il dataset disponibile al seguente URL:<br>\n",
    "https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset<br>\n",
    "Si costruisca un regressore lineare che predica i valori target a partire dagli altri attributi. Si provino\n",
    "diversi learning rate e numeri di epoche per far convergere il modello. Cosa succede nel caso di\n",
    "learning rate molto alti? E nel caso di learning rate molto bassi?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def reset_elements():\n",
    "    diabetes = load_diabetes()\n",
    "    X = diabetes.data\n",
    "    Y = diabetes.target\n",
    "    np.random.seed(123)\n",
    "    torch.random.manual_seed(123);\n",
    "    idx = np.random.permutation(len(X))\n",
    "    X = X[idx]\n",
    "    Y = Y[idx]\n",
    "    X_training = torch.Tensor(X[40:])\n",
    "    Y_training = torch.Tensor(Y[40:])\n",
    "    X_testing = torch.Tensor(X[:40])\n",
    "    Y_testing = torch.Tensor(Y[:40])\n",
    "    return X_training, Y_training, X_testing, Y_testing\n",
    "\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        \"\"\"Costruisce un regressore lineare.\n",
    "        Input:\n",
    "        in_size: numero di feature in input (es. 10)\n",
    "        out_size: numero di elementi in output (es. 1)\"\"\"\n",
    "        super(LinearRegressor, self).__init__() #richiamo il costruttore della superclasse\n",
    "        #questo passo è necessario per abilitare alcuni meccanismi automatici dei moduli di PyTorch\n",
    "        self.linear = nn.Linear(in_size,out_size)\n",
    "        self.means = 0\n",
    "        self.stds = 1\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.writer = SummaryWriter('logs/esercizio3/')\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        result = self.linear(x)\n",
    "        return result\n",
    "    \n",
    "    def _norm(self, X):\n",
    "        return (X-self.means)/self.stds\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(self._norm(X)).view(-1)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        return self.criterion(self.predict(X),Y).item()\n",
    "        \n",
    "    \n",
    "    def fit(self, X, Y, lr=0.01, epochs=100):\n",
    "        self.means = X.mean(0)\n",
    "        self.stds = X.std(0)\n",
    "        X_norm = self._norm(X)\n",
    "        optimizer = torch.optim.SGD(self.parameters(),lr=lr)\n",
    "        for e in range(epochs):\n",
    "            self.train()\n",
    "            output = self.forward(X_norm)\n",
    "            l = self.criterion(output.view(-1),Y)\n",
    "            #print(\"loss/fit: {}\".format(l.item()))\n",
    "            self.writer.add_scalar('lr={}/training'.format(lr), l.item(), global_step=e)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-10,11):\n",
    "    reg = LinearRegressor(10,1)\n",
    "    X_training, Y_training, X_testing, Y_testing = reset_elements()\n",
    "    reg.fit(X_training, Y_training, 1*(10**i), 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lr bassissimi -> arriva molto lentamente alla convergenza\n",
    "Lr altissimi >= 1 -> non converge mai, oscilla (o infinito)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Esercizio 4</h1>\n",
    "<p>\n",
    "    Si consideri il dataset visto nell'esercizio precedente. Si costruisca un regressore lineare per predire i\n",
    "valori delle variabili S1 , S2 , S3 a partire dai valori delle altre variabili. Per costruire il regressore,\n",
    "si specifichi\n",
    "3\n",
    "come numero di dimensioni in uscita. Si faccia attenzione al processo di\n",
    "normalizzazione dei dati. In questo caso, i valori delle variabili S1 , S2 , S3 non deve essere\n",
    "normalizzato. Qual è la loss finale sul test set?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def reset_elements():\n",
    "    diabetes = load_diabetes()\n",
    "    df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "    X = df[['age','sex','bmi','bp','s4','s5','s6']].copy().to_numpy()\n",
    "    Y = df[['s1','s2','s3']].copy().to_numpy()\n",
    "    np.random.seed(123)\n",
    "    torch.random.manual_seed(123);\n",
    "    idx = np.random.permutation(len(X))\n",
    "    X = X[idx]\n",
    "    Y = Y[idx]\n",
    "    X_training = torch.Tensor(X[40:])\n",
    "    Y_training = torch.Tensor(Y[40:])\n",
    "    X_testing = torch.Tensor(X[:40])\n",
    "    Y_testing = torch.Tensor(Y[:40])\n",
    "    return X_training, Y_training, X_testing, Y_testing\n",
    "\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        \"\"\"Costruisce un regressore lineare.\n",
    "        Input:\n",
    "        in_size: numero di feature in input (es. 10)\n",
    "        out_size: numero di elementi in output (es. 1)\"\"\"\n",
    "        super(LinearRegressor, self).__init__() #richiamo il costruttore della superclasse\n",
    "        #questo passo è necessario per abilitare alcuni meccanismi automatici dei moduli di PyTorch\n",
    "        self.linear = nn.Linear(in_size,out_size)\n",
    "        self.means = 0\n",
    "        self.stds = 1\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.writer = SummaryWriter('logs/esercizio4/')\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        result = self.linear(x)\n",
    "        return result\n",
    "    \n",
    "    def _norm(self, X):\n",
    "        return (X-self.means)/self.stds\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(self._norm(X)).view(-1)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        return self.criterion(self.predict(X),Y).item()\n",
    "        \n",
    "    \n",
    "    def fit(self, X, Y, lr=0.01, epochs=100):\n",
    "        self.means = X.mean(0)\n",
    "        self.stds = X.std(0)\n",
    "        X_norm = self._norm(X)\n",
    "        optimizer = torch.optim.SGD(self.parameters(),lr=lr)\n",
    "        for e in range(epochs):\n",
    "            self.train()\n",
    "            output = self.forward(X_norm)\n",
    "            l = self.criterion(output.view(-1),Y.view(-1))\n",
    "            #print(\"loss/fit: {}\".format(l.item()))\n",
    "            self.writer.add_scalar('lr={}/fit'.format(lr), l.item(), global_step=e)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegressor(7,3)\n",
    "X_training, Y_training, X_testing, Y_testing = reset_elements()\n",
    "reg.fit(X_training, Y_training, 1*(10**-1), 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
